"""
ElasticSearch RAG Pipeline: TripAdvisor ë¦¬ë·° ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ

BM25(í‚¤ì›Œë“œ) + ì‹œë§¨í‹±(ì„ë² ë”©) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ í†µí•´
ì‚¬ìš©ìì˜ ì¶”ìƒì ì¸ ìš”êµ¬ì‚¬í•­ì„ ì´í•´í•˜ê³  í˜¸í…”ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
"""

import os
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
# Optional/heavy dependencies: import lazily/with fallback so tests that only
# exercise lightweight parts (e.g. adaptive_alpha) don't fail at import time in CI.
try:
    import numpy as np
except Exception:
    np = None

from dataclasses import dataclass, asdict

try:
    from elasticsearch import Elasticsearch, helpers
except Exception:
    Elasticsearch = None
    helpers = None

try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None

try:
    import pandas as pd
except Exception:
    pd = None

try:
    from datasets import load_dataset
except Exception:
    load_dataset = None
from .re_ranker import simple_rerank
from .cross_reranker import try_cross_rerank

try:
    import nltk
    from nltk.corpus import wordnet
    WORDNET_AVAILABLE = True
    # WordNet ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì²˜ìŒ ì‹¤í–‰ ì‹œ)
    try:
        wordnet.synsets('test')
    except LookupError:
        nltk.download('wordnet', quiet=True)
        nltk.download('omw-1.4', quiet=True)
except ImportError:
    WORDNET_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ReviewDocument:
    """ë¦¬ë·° ë¬¸ì„œ ìŠ¤í‚¤ë§ˆ"""
    doc_id: str
    hotel_name: str
    location: str
    review_text: str
    rating: float
    review_title: Optional[str] = None
    reviewer_location: Optional[str] = None
    review_date: Optional[str] = None
    helpful_votes: int = 0
    total_votes: int = 0
    tags: List[str] = None
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {k: v for k, v in asdict(self).items() if v is not None}


class ElasticSearchRAG:
    """
    ElasticSearch ê¸°ë°˜ RAG ì‹œìŠ¤í…œ
    TripAdvisor ë¦¬ë·° ë°ì´í„°ë¥¼ ì¸ë±ì‹±í•˜ê³  í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    @staticmethod
    def _get_wordnet_synonyms(word: str, pos: str = None) -> List[str]:
        """
        WordNetì„ ì‚¬ìš©í•˜ì—¬ ë™ì˜ì–´ ì¶”ì¶œ
        
        Args:
            word: ë‹¨ì–´
            pos: í’ˆì‚¬ (noun, verb, adj, adv)
            
        Returns:
            ë™ì˜ì–´ ë¦¬ìŠ¤íŠ¸
        """
        if not WORDNET_AVAILABLE:
            return []
        
        synonyms = set()
        pos_tag = None
        if pos == 'noun':
            pos_tag = wordnet.NOUN
        elif pos == 'verb':
            pos_tag = wordnet.VERB
        elif pos == 'adj':
            pos_tag = wordnet.ADJ
        elif pos == 'adv':
            pos_tag = wordnet.ADV
        
        # ë™ì˜ì–´ ì§‘í•© ì¶”ì¶œ
        for syn in wordnet.synsets(word, pos=pos_tag):
            for lemma in syn.lemmas():
                # ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
                synonym = lemma.name().replace('_', ' ')
                if synonym.lower() != word.lower():
                    synonyms.add(synonym.lower())
        
        return list(synonyms)
    
    @staticmethod
    def _generate_hotel_synonyms() -> List[str]:
        """
        í˜¸í…”/ì—¬í–‰ ë„ë©”ì¸ íŠ¹í™” ë™ì˜ì–´ ìƒì„± (WordNet + ìˆ˜ë™ ì •ì˜)
        
        Returns:
            ë™ì˜ì–´ ë¦¬ìŠ¤íŠ¸ (Solr í˜•ì‹)
        """
        # ê¸°ë³¸ í˜¸í…”/ì—¬í–‰ ê´€ë ¨ ë™ì˜ì–´ (ìˆ˜ë™ ì •ì˜)
        base_synonyms = [
            # ë¶„ìœ„ê¸°/í™˜ê²½
            "quiet,peaceful,calm,tranquil,serene",
            "romantic,intimate,cozy,charming",
            "luxury,luxurious,premium,upscale,high-end,deluxe,elegant,sophisticated",
            "budget,cheap,affordable,economical,inexpensive",
            
            # ì²­ê²°
            "clean,tidy,spotless,pristine,immaculate",
            "dirty,unclean,filthy,messy",
            
            # ì„œë¹„ìŠ¤
            "friendly,hospitable,welcoming,warm,courteous,pleasant,helpful",
            "rude,unfriendly,impolite,discourteous",
            "professional,competent,efficient,skilled",
            
            # ìœ„ì¹˜
            "central,downtown,city center",
            "nearby,close,near,adjacent",
            "remote,isolated,far,distant",
            
            # ì‹œì„¤
            "breakfast,morning meal",
            "wifi,internet,wireless,wi-fi",
            "pool,swimming pool",
            "gym,fitness center,workout room,exercise room",
            "spa,wellness center",
            "parking,car park,garage",
            "restaurant,dining,eatery",
            "bar,lounge,pub",
            
            # ê°ì‹¤
            "room,suite,accommodation,chamber",
            "spacious,large,roomy,big,ample",
            "tiny,small,cramped,compact",
            "comfortable,cozy",
            "view,scenery,vista",
            "balcony,terrace,patio",
            
            # ê°€ê²©
            "expensive,costly,pricey,overpriced",
            "reasonable,fair",
            
            # ìŒì‹
            "delicious,tasty,yummy",
            
            # ìƒíƒœ
            "modern,contemporary,updated,renovated",
            "old,dated,outdated,worn",
            "new,brand new,fresh",
            
            # ì†ŒìŒ
            "noisy,loud,disturbing",
            
            # ì—¬í–‰ íƒ€ì…
            "family friendly,kid friendly",
            "business hotel,business center",
            "pet friendly,pets allowed,dog friendly",
            
            # í’ˆì§ˆ
            "excellent,outstanding,exceptional,superb,fantastic,wonderful,great",
            "poor,bad,terrible,awful,disappointing",
            "good,nice,pleasant,satisfactory",
            "average,okay,mediocre,decent"
        ]
        
        # WordNetìœ¼ë¡œ ì¶”ê°€ ë™ì˜ì–´ í™•ì¥ (ì„ íƒì )
        if WORDNET_AVAILABLE:
            logger.info("WordNetì„ ì‚¬ìš©í•˜ì—¬ ë™ì˜ì–´ í™•ì¥ ì¤‘...")
            
            # ì´ë¯¸ ìˆ˜ë™ ì •ì˜ëœ ë‹¨ì–´ë“¤ (ì¤‘ë³µ ë°©ì§€)
            manually_defined = {
                'quiet', 'romantic', 'luxury', 'budget', 'clean', 'dirty',
                'friendly', 'rude', 'professional', 'spacious', 'comfortable',
                'excellent', 'poor', 'good', 'average', 'wonderful'
            }
            
            # ì¤‘ìš” ë‹¨ì–´ë“¤ì— ëŒ€í•´ WordNet ë™ì˜ì–´ ì¶”ê°€
            important_words = {
                'beautiful': 'adj',
                'convenient': 'adj',
                'amazing': 'adj',
                'perfect': 'adj',
                'helpful': 'adj',
            }
            
            for word, pos in important_words.items():
                # ìˆ˜ë™ ì •ì˜ëœ ë‹¨ì–´ëŠ” ê±´ë„ˆë›°ê¸°
                if word in manually_defined:
                    continue
                    
                synonyms = ElasticSearchRAG._get_wordnet_synonyms(word, pos)
                if synonyms and len(synonyms) > 0:
                    # ë„ˆë¬´ ë§ìœ¼ë©´ ìƒìœ„ 5ê°œë§Œ
                    synonym_str = f"{word},{','.join(synonyms[:5])}"
                    base_synonyms.append(synonym_str)
                    logger.debug(f"WordNet synonyms for '{word}': {synonyms[:5]}")
        
        return base_synonyms
    
    def __init__(
        self,
        es_host: str = "localhost",
        es_port: int = 9200,
        es_user: str = "elastic",       # [ì¶”ê°€] ê¸°ë³¸ ì‚¬ìš©ì
        es_password: str = "changeme",  # [ì¶”ê°€] ê¸°ë³¸ ë¹„ë°€ë²ˆí˜¸
        index_name: str = "art_hotel_reviews",
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"
    ):
        """
        ì´ˆê¸°í™”
        Args:
            es_host: ElasticSearch í˜¸ìŠ¤íŠ¸
            es_port: ElasticSearch í¬íŠ¸
            es_user: ì‚¬ìš©ì ID
            es_password: ì‚¬ìš©ì ë¹„ë°€ë²ˆí˜¸
            index_name: ì¸ë±ìŠ¤ ì´ë¦„
            embedding_model: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        """
        
        # ElasticSearch ì—°ê²°
        self.es = Elasticsearch(
            [{'host': es_host, 'port': es_port, 'scheme': 'http'}],
            basic_auth=(es_user, es_password),  # [í•µì‹¬ ìˆ˜ì •] ì¸ì¦ ì •ë³´ ì „ë‹¬
            request_timeout=30,
            max_retries=3,
            retry_on_timeout=True
        )
        '''
        # ElasticSearch ì—°ê²° (ë³´ì•ˆ ë¹„í™œì„±í™” ëª¨ë“œ)
        self.es = Elasticsearch(
            [{'host': es_host, 'port': es_port, 'scheme': 'http'}],
            timeout=30,
            max_retries=3,
            retry_on_timeout=True
        )
        '''
        
        self.index_name = index_name
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        
        logger.info(f"ElasticSearch RAG ì´ˆê¸°í™” ì™„ë£Œ: {es_host}:{es_port} (User: {es_user})")
    
    def create_index(self, force_recreate: bool = False):
        """
        ElasticSearch ì¸ë±ìŠ¤ ìƒì„±
        
        Args:
            force_recreate: Trueë©´ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ í›„ ì¬ìƒì„±
        """
        
        if force_recreate and self.es.indices.exists(index=self.index_name):
            logger.warning(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ: {self.index_name}")
            self.es.indices.delete(index=self.index_name)
        
        if not self.es.indices.exists(index=self.index_name):
            # ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜
            mapping = {
                "mappings": {
                    "properties": {
                        # ê¸°ë³¸ í•„ë“œ
                        "doc_id": {"type": "keyword"},
                        "hotel_name": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
                        "location": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
                        
                        # ë¦¬ë·° í…ìŠ¤íŠ¸ (BM25 ê²€ìƒ‰ìš©)
                        "review_text": {
                            "type": "text",
                            "analyzer": "standard",  # ë˜ëŠ” custom analyzer
                            "fields": {
                                "english": {"type": "text", "analyzer": "english"}
                            }
                        },
                        "review_title": {"type": "text"},
                        
                        # ë©”íƒ€ë°ì´í„°
                        "rating": {"type": "float"},
                        "review_date": {"type": "date", "format": "yyyy-MM-dd||epoch_millis", "ignore_malformed": True},
                        "reviewer_location": {"type": "keyword"},
                        "helpful_votes": {"type": "integer"},
                        "total_votes": {"type": "integer"},
                        "tags": {"type": "keyword"},
                        
                        # ì„ë² ë”© ë²¡í„° (ì‹œë§¨í‹± ê²€ìƒ‰ìš©)
                        "review_vector": {
                            "type": "dense_vector",
                            "dims": self.embedding_dim,
                            "index": True,
                            "similarity": "cosine"
                        },
                        
                        # ì§‘ê³„ëœ í˜¸í…” ì •ë³´
                        "hotel_avg_rating": {"type": "float"},
                        "hotel_review_count": {"type": "integer"},
                        
                        # ì¸ë±ì‹± íƒ€ì„ìŠ¤íƒ¬í”„
                        "indexed_at": {"type": "date"}
                    }
                },
                "settings": {
                    "number_of_shards": 2,
                    "number_of_replicas": 1,
                    "analysis": {
                        "analyzer": {
                            "review_analyzer": {
                                "type": "custom",
                                "tokenizer": "standard",
                                "filter": ["lowercase", "stop", "synonym_filter", "stemmer"]
                            }
                        },
                        "filter": {
                            "synonym_filter": {
                                "type": "synonym",
                                "synonyms": self._generate_hotel_synonyms()
                            }
                        }
                    }
                }
            }
            
            # ì¸ë±ìŠ¤ ìƒì„±
            self.es.indices.create(index=self.index_name, body=mapping)
            logger.info(f"ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {self.index_name}")
        else:
            logger.info(f"ì¸ë±ìŠ¤ ì´ë¯¸ ì¡´ì¬: {self.index_name}")
    
    def load_and_process_tripadvisor_data(self, max_docs: int = None) -> List[ReviewDocument]:
        """
        HuggingFaceì—ì„œ TripAdvisor ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
        
        Args:
            max_docs: ë¡œë“œí•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
            
        Returns:
            ReviewDocument ë¦¬ìŠ¤íŠ¸
        """
        
        logger.info("TripAdvisor ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘...")
        
        # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ
        dataset = load_dataset("jniimi/tripadvisor-review-rating", split="train")
        
        if max_docs:
            dataset = dataset.select(range(min(max_docs, len(dataset))))
        
        documents = []
        
        for idx, item in enumerate(dataset):
            try:
                # í•„ë“œ ë§¤í•‘ (ì‹¤ì œ ë°ì´í„°ì…‹ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
                doc = ReviewDocument(
                    doc_id=f"tripadvisor_{idx}",
                    hotel_name=item.get('hotel_name', f"Hotel_{idx}"),  # ì‹¤ì œ í•„ë“œëª… í™•ì¸ í•„ìš”
                    location=item.get('location', 'Unknown'),
                    review_text=item.get('text', item.get('review', '')),  # ë¦¬ë·° í…ìŠ¤íŠ¸ í•„ë“œ
                    rating=float(item.get('rating', 0)),
                    review_title=item.get('title', None),
                    tags=self._extract_tags(item.get('text', ''))
                )
                
                documents.append(doc)
                
                if idx % 1000 == 0:
                    logger.info(f"ì²˜ë¦¬ ì§„í–‰: {idx}/{len(dataset)}")
                    
            except Exception as e:
                logger.warning(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨ (idx={idx}): {str(e)}")
                continue
        
        logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
        return documents
    
    def _extract_tags(self, text: str) -> List[str]:
        """ë¦¬ë·° í…ìŠ¤íŠ¸ì—ì„œ íƒœê·¸ ì¶”ì¶œ"""
        tags = []
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ íƒœê·¸ ì¶”ì¶œ
        keyword_tags = {
            'wifi': ['wifi', 'internet', 'wireless'],
            'breakfast': ['breakfast', 'morning meal'],
            'parking': ['parking', 'car park'],
            'pool': ['pool', 'swimming'],
            'gym': ['gym', 'fitness', 'workout'],
            'spa': ['spa', 'massage', 'wellness'],
            'pet_friendly': ['pet', 'dog', 'cat'],
            'business': ['business', 'conference', 'meeting room'],
            'family': ['family', 'kids', 'children'],
            'romantic': ['romantic', 'honeymoon', 'couples']
        }
        
        text_lower = text.lower()
        for tag, keywords in keyword_tags.items():
            if any(keyword in text_lower for keyword in keywords):
                tags.append(tag)
        
        return tags
        
    def index_documents(self, documents: List[ReviewDocument], batch_size: int = 2000, use_dummy_embedding: bool = True):
        """
        ë¬¸ì„œ ì¸ë±ì‹± í•¨ìˆ˜
        
        Args:
            documents: ì¸ë±ì‹±í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 2000)
            use_dummy_embedding: 
                - True: ëœë¤ ë²¡í„° ì‚¬ìš© (ì†ë„ ë¹ ë¦„, ì‹œìŠ¤í…œ ì—°ë™ í…ŒìŠ¤íŠ¸ìš©, ì‹œë§¨í‹± ê²€ìƒ‰ ë¶ˆê°€)
                - False: ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš© (ì†ë„ ëŠë¦¼, ì‹¤ì œ ì„œë¹„ìŠ¤ìš©, ì‹œë§¨í‹± ê²€ìƒ‰ ê°€ëŠ¥)
        """
        import numpy as np # numpyê°€ ì—†ë‹¤ë©´ ìƒë‹¨ì— import í•„ìš”

        mode_str = "ğŸš€ ë”ë¯¸(ëœë¤) ë²¡í„°" if use_dummy_embedding else "ğŸ§  ì‹¤ì œ AI ì„ë² ë”©"
        logger.info(f"ì¸ë±ì‹± ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ (ëª¨ë“œ: {mode_str})")
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i+batch_size]
            
            # ---------------------------------------------------------
            # [ëª¨ë“œ ì „í™˜] ì„ë² ë”© ìƒì„± ë°©ì‹ ì„ íƒ
            # ---------------------------------------------------------
            if use_dummy_embedding:
                # [Fast Mode] 0.0 ~ 1.0 ì‚¬ì´ì˜ ëœë¤ ë²¡í„° ìƒì„±
                # CPU ë¶€í•˜ê°€ ê±°ì˜ ì—†ìœ¼ë©°, 0 ë²¡í„° ì—ëŸ¬(Cosine Similarity Error)ë¥¼ ë°©ì§€í•¨
                embeddings = np.random.rand(len(batch), self.embedding_dim)
            else:
                # [Real Mode] ì‹¤ì œ SentenceTransformer ëª¨ë¸ë¡œ ì„ë² ë”© ìƒì„±
                # CPU/GPU ì—°ì‚°ì´ í•„ìš”í•˜ë©° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼
                texts = [doc.review_text for doc in batch]
                embeddings = self.embedding_model.encode(texts, show_progress_bar=False)
            # ---------------------------------------------------------

            # ElasticSearch bulk ì‘ì—… ì¤€ë¹„
            actions = []
            for doc, embedding in zip(batch, embeddings):
                action = {
                    "_index": self.index_name,
                    "_id": doc.doc_id,
                    "_source": {
                        **doc.to_dict(),
                        "review_vector": embedding.tolist(),
                        "indexed_at": datetime.now().isoformat()
                    }
                }
                actions.append(action)
            
            # [ë””ë²„ê¹… & ì‹¤í–‰] stats_only=Falseë¡œ ì„¤ì •í•˜ì—¬ ìƒì„¸ ì—ëŸ¬ í™•ì¸
            try:
                success, items = helpers.bulk(
                    self.es,
                    actions,
                    stats_only=False,  # ìƒì„¸ ì—ëŸ¬ í™•ì¸ì„ ìœ„í•´ False ì„¤ì •
                    raise_on_error=False,
                    request_timeout=60 # ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ë¥¼ ìœ„í•´ íƒ€ì„ì•„ì›ƒ ì—¬ìœ  ìˆê²Œ ì„¤ì •
                )
                
                # ì‹¤íŒ¨í•œ í•­ëª©ì´ ìˆë‹¤ë©´ ì²« ë²ˆì§¸ ì—ëŸ¬ ì›ì¸ì„ ë¡œê·¸ì— ì¶œë ¥
                failed_items = [item for item in items if item.get('index', {}).get('error')]
                if failed_items:
                    first_error = failed_items[0]['index']['error']
                    logger.error(f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨ ì›ì¸ (ì²«ë²ˆì§¸ í•­ëª©): {json.dumps(first_error, indent=2, ensure_ascii=False)}")
                    logger.info(f"ë°°ì¹˜ ì¸ë±ì‹±: ì„±ê³µ={success}, ì‹¤íŒ¨={len(failed_items)}")
                else:
                    logger.info(f"ë°°ì¹˜ ì¸ë±ì‹±: ì„±ê³µ={success}, ì‹¤íŒ¨=0")
                    
            except Exception as e:
                logger.error(f"Bulk ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")
        
        # ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨
        self.es.indices.refresh(index=self.index_name)
        logger.info("ì¸ë±ì‹± ë¡œì§ ì¢…ë£Œ")



    
    def hybrid_search(
        self,
        query: str,
        location: Optional[str] = None,
        min_rating: Optional[float] = None,
        tags: Optional[List[str]] = None,
        top_k: int = 10,
        alpha: Optional[float] = None
    ) -> List[Dict[str, Any]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + ì‹œë§¨í‹±)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            location: ìœ„ì¹˜ í•„í„°
            min_rating: ìµœì†Œ í‰ì 
            tags: íƒœê·¸ í•„í„°
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            alpha: í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜ (0=BM25ë§Œ, 1=ì‹œë§¨í‹±ë§Œ)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        
        # í•„í„° êµ¬ì„±
        filters = []
        if location:
            filters.append({"match": {"location": location}})
        if min_rating:
            filters.append({"range": {"rating": {"gte": min_rating}}})
        if tags:
            filters.append({"terms": {"tags": tags}})
        
        # If alpha not provided, compute an adaptive alpha based on query characteristics
        if alpha is None:
            alpha = self.adaptive_alpha(query)

        # 1. BM25 ê²€ìƒ‰
        bm25_query = {
            "bool": {
                "must": [
                    {
                        "multi_match": {
                            "query": query,
                            "fields": ["review_text^2", "review_title", "hotel_name"],
                            "type": "best_fields"
                        }
                    }
                ],
                "filter": filters
            }
        }
        
        bm25_results = self.es.search(
            index=self.index_name,
            body={
                "query": bm25_query,
                "size": top_k * 2  # ë” ë§ì´ ê²€ìƒ‰í•´ì„œ ë‚˜ì¤‘ì— ìœµí•©
            }
        )
        
        # 2. ì‹œë§¨í‹± ê²€ìƒ‰
        query_embedding = self.embedding_model.encode(query, show_progress_bar=False)
        
        semantic_query = {
            "script_score": {
                "query": {
                    "bool": {
                        "filter": filters
                    }
                },
                "script": {
                    "source": "cosineSimilarity(params.query_vector, 'review_vector') + 1.0",
                    "params": {"query_vector": query_embedding.tolist()}
                }
            }
        }
        
        semantic_results = self.es.search(
            index=self.index_name,
            body={
                "query": semantic_query,
                "size": top_k * 2
            }
        )
        
        # 3. ê²°ê³¼ ìœµí•©
        combined_results = self._fuse_results(
            bm25_results['hits']['hits'],
            semantic_results['hits']['hits'],
            alpha=alpha
        )

        # 4.5 Optional re-ranking hook (placeholder for cross-encoder or other models)
        combined_results = self.rerank_results(combined_results, query)
        
        # 4. ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for result in combined_results[:top_k]:
            source = result['_source']
            formatted_results.append({
                'hotel_name': source['hotel_name'],
                'location': source['location'],
                'rating': source['rating'],
                'review_snippet': source['review_text'][:200] + '...',
                'tags': source.get('tags', []),
                'combined_score': result['combined_score'],
                'bm25_score': result.get('bm25_score', 0),
                'semantic_score': result.get('semantic_score', 0)
            })
        
        return formatted_results

    def adaptive_alpha(self, query: str) -> float:
        """
        ê°„ë‹¨í•œ ì¿¼ë¦¬ ë¶„ì„ìœ¼ë¡œ ë™ì  alphaë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        - 'romantic', 'quiet' ë“± ë¶„ìœ„ê¸° í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì‹œë§¨í‹±(ë²¡í„°) ê°€ì¤‘ì¹˜ë¥¼ ë†’ì„
        - ëª…ì‹œì  í‚¤ì›Œë“œ(ì˜ˆ: 'near', 'center', 'breakfast')ê°€ ë§ìœ¼ë©´ BM25 ê°€ì¤‘ì¹˜ë¥¼ ë†’ì„
        """
        q = query.lower()
        semantic_keywords = ['romantic', 'quiet', 'cozy', 'intimate', 'relax', 'luxury', 'scenic']
        keyword_indicators = ['near', 'nearby', 'center', 'close', 'breakfast', 'parking', 'pool']

        semantic_score = sum(1 for k in semantic_keywords if k in q)
        keyword_score = sum(1 for k in keyword_indicators if k in q)

        if semantic_score > keyword_score:
            return min(0.9, 0.6 + 0.1 * (semantic_score - keyword_score))
        elif keyword_score > semantic_score:
            return max(0.1, 0.4 - 0.1 * (keyword_score - semantic_score))
        else:
            return 0.5

    def rerank_results(self, results: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """
        Re-ranking hook. í˜„ì¬ëŠ” placeholderë¡œ ì…ë ¥ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        í–¥í›„ Cross-Encoderë‚˜ í•™ìŠµëœ ìˆœìœ„ëª¨ë¸ì„ ì ìš©í•  ìˆ˜ ìˆìŒ.
        """
        # Try optional cross-encoder reranker first (enabled via env USE_CROSS_ENCODER).
        try:
            cross = try_cross_rerank(results, query)
            if cross is not None:
                return cross
        except Exception:
            # non-fatal: fall back to lightweight reranker
            pass

        # Fallback: lightweight lexical reranker
        try:
            return simple_rerank(results, query)
        except Exception:
            return results
    
    def _fuse_results(
        self,
        bm25_results: List[Dict],
        semantic_results: List[Dict],
        alpha: float = 0.5
    ) -> List[Dict]:
        """
        BM25ì™€ ì‹œë§¨í‹± ê²€ìƒ‰ ê²°ê³¼ ìœµí•©
        
        Args:
            bm25_results: BM25 ê²€ìƒ‰ ê²°ê³¼
            semantic_results: ì‹œë§¨í‹± ê²€ìƒ‰ ê²°ê³¼
            alpha: ê°€ì¤‘ì¹˜ (0=BM25ë§Œ, 1=ì‹œë§¨í‹±ë§Œ)
            
        Returns:
            ìœµí•©ëœ ê²°ê³¼
        """
        
        # ì ìˆ˜ ì •ê·œí™”
        max_bm25 = max([r['_score'] for r in bm25_results]) if bm25_results else 1
        max_semantic = max([r['_score'] for r in semantic_results]) if semantic_results else 1
        
        # ê²°ê³¼ ë§µ ìƒì„±
        result_map = {}
        
        # BM25 ê²°ê³¼ ì²˜ë¦¬
        for r in bm25_results:
            doc_id = r['_id']
            result_map[doc_id] = {
                **r,
                'bm25_score': r['_score'] / max_bm25,
                'semantic_score': 0
            }
        
        # ì‹œë§¨í‹± ê²°ê³¼ ì²˜ë¦¬
        for r in semantic_results:
            doc_id = r['_id']
            if doc_id in result_map:
                result_map[doc_id]['semantic_score'] = r['_score'] / max_semantic
            else:
                result_map[doc_id] = {
                    **r,
                    'bm25_score': 0,
                    'semantic_score': r['_score'] / max_semantic
                }
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        for doc_id in result_map:
            result_map[doc_id]['combined_score'] = (
                (1 - alpha) * result_map[doc_id]['bm25_score'] +
                alpha * result_map[doc_id]['semantic_score']
            )
        
        # ì •ë ¬
        sorted_results = sorted(
            result_map.values(),
            key=lambda x: x['combined_score'],
            reverse=True
        )
        
        return sorted_results
    
    def get_similar_hotels(self, hotel_name: str, top_k: int = 5) -> List[Dict]:
        """
        ìœ ì‚¬í•œ í˜¸í…” ì°¾ê¸° (ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜)
        
        Args:
            hotel_name: ê¸°ì¤€ í˜¸í…” ì´ë¦„
            top_k: ë°˜í™˜í•  í˜¸í…” ìˆ˜
            
        Returns:
            ìœ ì‚¬ í˜¸í…” ë¦¬ìŠ¤íŠ¸
        """
        
        # ê¸°ì¤€ í˜¸í…” ê²€ìƒ‰
        base_query = {
            "match": {"hotel_name.keyword": hotel_name}
        }
        
        base_result = self.es.search(
            index=self.index_name,
            body={"query": base_query, "size": 1}
        )
        
        if not base_result['hits']['hits']:
            logger.warning(f"í˜¸í…”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {hotel_name}")
            return []
        
        # ê¸°ì¤€ ë²¡í„° ì¶”ì¶œ
        base_vector = base_result['hits']['hits'][0]['_source']['review_vector']
        
        # ìœ ì‚¬ í˜¸í…” ê²€ìƒ‰
        similar_query = {
            "script_score": {
                "query": {
                    "bool": {
                        "must_not": [
                            {"match": {"hotel_name.keyword": hotel_name}}
                        ]
                    }
                },
                "script": {
                    "source": "cosineSimilarity(params.query_vector, 'review_vector') + 1.0",
                    "params": {"query_vector": base_vector}
                }
            }
        }
        
        similar_results = self.es.search(
            index=self.index_name,
            body={
                "query": similar_query,
                "size": top_k,
                "aggs": {
                    "unique_hotels": {
                        "terms": {
                            "field": "hotel_name.keyword",
                            "size": top_k
                        }
                    }
                }
            }
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        hotels = []
        seen_hotels = set()
        
        for hit in similar_results['hits']['hits']:
            hotel = hit['_source']['hotel_name']
            if hotel not in seen_hotels:
                hotels.append({
                    'hotel_name': hotel,
                    'location': hit['_source']['location'],
                    'rating': hit['_source']['rating'],
                    'similarity_score': hit['_score']
                })
                seen_hotels.add(hotel)
        
        return hotels
    
    def analyze_hotel_reviews(self, hotel_name: str) -> Dict[str, Any]:
        """
        íŠ¹ì • í˜¸í…”ì˜ ë¦¬ë·° ë¶„ì„
        
        Args:
            hotel_name: í˜¸í…” ì´ë¦„
            
        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        
        # í˜¸í…” ë¦¬ë·° ê²€ìƒ‰
        query = {
            "match": {"hotel_name": hotel_name}
        }
        
        results = self.es.search(
            index=self.index_name,
            body={
                "query": query,
                "size": 100,
                "aggs": {
                    "avg_rating": {"avg": {"field": "rating"}},
                    "rating_distribution": {
                        "histogram": {
                            "field": "rating",
                            "interval": 1
                        }
                    },
                    "common_tags": {
                        "terms": {
                            "field": "tags",
                            "size": 10
                        }
                    }
                }
            }
        )
        
        # ë¶„ì„ ê²°ê³¼ êµ¬ì„±
        analysis = {
            'hotel_name': hotel_name,
            'total_reviews': results['hits']['total']['value'],
            'avg_rating': results['aggregations']['avg_rating']['value'],
            'rating_distribution': [
                {'rating': b['key'], 'count': b['doc_count']}
                for b in results['aggregations']['rating_distribution']['buckets']
            ],
            'common_tags': [
                {'tag': b['key'], 'count': b['doc_count']}
                for b in results['aggregations']['common_tags']['buckets']
            ],
            'sample_reviews': [
                {
                    'rating': hit['_source']['rating'],
                    'text': hit['_source']['review_text'][:200] + '...'
                }
                for hit in results['hits']['hits'][:3]
            ]
        }
        
        return analysis


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_rag_instance = None

def get_rag_instance() -> ElasticSearchRAG:
    """RAG ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rag_instance
    if _rag_instance is None:
        # í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
        host = os.getenv("ES_HOST", "localhost")
        port = int(os.getenv("ES_PORT", 9200))
        user = os.getenv("ES_USER", "elastic")         # [ì¶”ê°€]
        password = os.getenv("ES_PASSWORD", "changeme") # [ì¶”ê°€]
        
        _rag_instance = ElasticSearchRAG(
            es_host=host,
            es_port=port,
            es_user=user,          # [ì¶”ê°€]
            es_password=password   # [ì¶”ê°€]
        )
    return _rag_instance
