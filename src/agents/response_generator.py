"""
Response Generator Agent: ìµœì¢… ì‘ë‹µ ìƒì„± ì—ì´ì „íŠ¸ (Powered by Gemini)
"""
import logging
from typing import Dict, Any, List
from datetime import datetime

import os
try:
    # Optional LLM binding (only loaded when ENABLE_LLM=1)
    from langchain_google_genai import ChatGoogleGenerativeAI
except Exception:
    ChatGoogleGenerativeAI = None


logger = logging.getLogger(__name__)

class ResponseGeneratorAgent:
    def __init__(self):
        # LLM initialization is optional and gated by environment to keep tests
        # and CI lightweight. Set ENABLE_LLM=1 to attempt to use Google Gemini.
        self.llm = None
        self.enable_llm = os.getenv('ENABLE_LLM', '').lower() in ('1', 'true', 'yes') and ChatGoogleGenerativeAI is not None
        if self.enable_llm:
            try:
                self.llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0.7)
            except Exception:
                self.llm = None

        # Use a simple string template for prompt formatting to avoid hard
        # dependency on langchain prompt classes in tests.
        self.prompt_template = (
            "ë‹¹ì‹ ì€ ê°ê°ì ì´ê³  ì „ë¬¸ì ì¸ ì—¬í–‰ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ê°€ë…ì„± ë†’ê³  ë§¤ë ¥ì ì¸ ì—¬í–‰ ê³„íšì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n"
            
            "**[ì§€ì¹¨]**\n"
            "1. **í†¤ì•¤ë§¤ë„ˆ**: ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì´ë©°, ì—¬í–‰ì˜ ì„¤ë ˜ì´ ëŠê»´ì§€ë„ë¡ ì‘ì„±í•˜ì„¸ìš”.\n"
            "2. **ê°€ë…ì„±(Formatting)**: \n"
            "   - ëª¨ë“  ì„¹ì…˜ ì œëª©(Header) ì•ì—ëŠ” ë°˜ë“œì‹œ ê´€ë ¨ëœ **ì´ëª¨ì§€**ë¥¼ ë¶™ì´ì„¸ìš”. (ì˜ˆ: ## âœˆï¸ í•­ê³µí¸, ## ğŸ¨ ìˆ™ì†Œ)\n"
            "   - ì¤‘ìš” ì •ë³´ëŠ” **ë³¼ë“œì²´**ë¡œ ê°•ì¡°í•˜ì„¸ìš”.\n"
            "   - ì¼ì°¨ë³„ ì¼ì •ì€ íƒ€ì„ë¼ì¸ í˜•íƒœë‚˜ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(Bullet points)ë¥¼ ì‚¬ìš©í•´ ì •ë¦¬í•˜ì„¸ìš”.\n"
            "3. **ì´ëª¨ì§€ ì‚¬ìš© ì›ì¹™**: ë¬¸ë§¥ì— ê°€ì¥ ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ ë‹¹ì‹ ì´ ììœ ë¡­ê²Œ ì„ íƒí•˜ë˜, ê³¼í•˜ì§€ ì•Šê²Œ ì ì¬ì ì†Œì— ë°°ì¹˜í•˜ì„¸ìš”.\n"
            "4. **ë‚ ì”¨ ì •ë³´ ì²˜ë¦¬**: ë‚ ì”¨ ì˜ˆë³´ê°€ Markdown í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì œê³µë˜ë©´ **ë°˜ë“œì‹œ ê·¸ëŒ€ë¡œ ìœ ì§€**í•˜ì„¸ìš”. í…Œì´ë¸”ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì§€ ë§ˆì„¸ìš”.\n\n"

            "**[ì‘ì„± í¬ë§· ì˜ˆì‹œ]**\n"
            "## ğŸŒ¤ï¸ ë‚ ì”¨ ì˜ˆë³´\n"
            "| ë‚ ì§œ | ë‚ ì”¨ | ìµœì €ê¸°ì˜¨ | ìµœê³ ê¸°ì˜¨ | ê°•ìˆ˜ëŸ‰ |\n"
            "|------|------|----------|----------|--------|\n"
            "| 2025-12-01 | ì•½í•œ ë¹„ | 4.5Â°C | 9.1Â°C | 4.1mm |\n\n"
            "## ğŸ¨ ì¶”ì²œ ìˆ™ì†Œ: [í˜¸í…”ëª…]\n"
            "...\n"
            "### ğŸ“… 1ì¼ì°¨: [í…Œë§ˆ]\n"
            "- **ì˜¤ì „ (09:00)**: ğŸ›ï¸ [ì¥ì†Œëª…] ë°©ë¬¸\n"
            "- **ì ì‹¬**: ğŸ½ï¸ [ì‹ë‹¹ëª…]ì—ì„œ í˜„ì§€ì‹ ì¦ê¸°ê¸°\n\n"
            
            "**[ì…ë ¥ ë°ì´í„°]**\n"
            "- ëª©ì ì§€: {destination}\n"
            "- ì—¬í–‰ ë‚ ì§œ: {dates}\n"
            "- ì—¬í–‰ ì¸ì›: {traveler_count}ëª…\n"
            "- ì„ í˜¸ì‚¬í•­: {preferences}\n"
            "- í˜¸í…” ì •ë³´: {hotel_results}\n"
            "- ë‚ ì”¨ ì˜ˆë³´: {weather_forecast}\n"
            "- ê²€ìƒ‰ ì •ë³´: {google_info}\n\n"
            
            "**[ì¤‘ìš”]** ë‚ ì”¨ ì˜ˆë³´ê°€ í…Œì´ë¸” í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”. ì ˆëŒ€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
            "ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—¬í–‰ìê°€ ë°”ë¡œ ë– ë‚˜ê³  ì‹¶ì–´ì§€ë„ë¡ ìƒì„¸í•œ ê³„íšì„ ì‘ì„±í•´ì£¼ì„¸ìš”."
        )
        logger.info("ResponseGeneratorAgent(Gemini) ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def generate(self, state: Dict[str, Any]) -> Dict[str, Any]:
        try:
            destination = state.get('destination', 'ëª©ì ì§€ ë¯¸ì •')
            dates = state.get('travel_dates', ['ë‚ ì§œ ë¯¸ì •'])
            
            # ë‚ ì”¨ ì •ë³´ í¬ë§·íŒ…
            weather_info = self._format_weather_forecast(
                state.get('weather_forecast', []),
                state.get('context_memory', {}).get('weather_limitation_message')
            )
            
            prompt_text = self.prompt_template.format(
                destination=destination,
                dates=' ~ '.join(dates) if isinstance(dates, list) else dates,
                traveler_count=state.get('traveler_count', 1),
                preferences=str(state.get('preferences', {})),
                hotel_results=self._format_hotel_results(state.get('hotel_options', [])),
                weather_forecast=weather_info,
                google_info=str(state.get('google_search_results', []))
            )

            if self.llm:
                try:
                    response = await self.llm.ainvoke(prompt_text)
                    content = getattr(response, 'content', str(response))
                except Exception:
                    content = "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                # Lightweight fallback summary (non-LLM)
                content = f"{destination}ì— ëŒ€í•œ ê°„ë‹¨í•œ ì¼ì •ì…ë‹ˆë‹¤. í˜¸í…” {len(state.get('hotel_options', []))}ê³³ ì¶”ì²œ." 

            return {
                'destination': destination,
                'dates': dates,
                'summary': content,
                'hotels': [{'name': h.name, 'rating': h.rating, 'price': h.price_range, 'highlights': h.review_highlights} for h in state.get('hotel_options', [])[:3]],
                'generated_at': datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {'summary': f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", 'hotels': []}

    def _format_hotel_results(self, hotels: List) -> str:
        if not hotels: return "ê²€ìƒ‰ëœ í˜¸í…” ì—†ìŒ"
        return "\n".join([f"- {h.name} (í‰ì : {h.rating}, ê°€ê²©: {h.price_range})" for h in hotels[:3]])
    
    def _format_weather_forecast(self, forecasts: List, limitation_message: str = None) -> str:
        """
        ë‚ ì”¨ ì˜ˆë³´ë¥¼ Markdown í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        """
        if not forecasts:
            if limitation_message:
                return f"âš ï¸ {limitation_message}"
            return "ë‚ ì”¨ ì •ë³´ ì—†ìŒ (ë‚ ì§œê°€ 2ì£¼ ì´í›„ì´ê±°ë‚˜ ì¡°íšŒ ì‹¤íŒ¨)"
        
        table = "| ë‚ ì§œ | ë‚ ì”¨ | ìµœì €ê¸°ì˜¨ | ìµœê³ ê¸°ì˜¨ | ê°•ìˆ˜ëŸ‰ |\n"
        table += "|------|------|----------|----------|--------|\n"
        
        for forecast in forecasts:
            table += f"| {forecast.date} | {forecast.description} | {forecast.temperature_min}Â°C | {forecast.temperature_max}Â°C | {forecast.precipitation}mm |\n"
        
        return table

    async def stream_response(self, state: Dict[str, Any]):
        """Async generator that yields parts of the response incrementally.

        This is a lightweight PoC for streaming responses without invoking the
        LLM for every partial update. It yields three steps:
        1) hotels list
        2) weather table or message
        3) final summary (placeholder if LLM not available)
        """
        # 1) hotels
        hotels = [{'name': h.name, 'rating': h.rating, 'price': h.price_range, 'highlights': h.review_highlights} for h in state.get('hotel_options', [])[:3]]
        yield {'step': 'hotels', 'hotels': hotels}

        # 2) weather
        weather_info = self._format_weather_forecast(
            state.get('weather_forecast', []),
            state.get('context_memory', {}).get('weather_limitation_message')
        )
        yield {'step': 'weather', 'weather': weather_info}

        # 3) final summary â€” try LLM generate, but fall back to a simple placeholder
        try:
            final = await self.generate(state)
            yield {'step': 'final', 'itinerary': final}
        except Exception:
            # Non-fatal fallback
            yield {'step': 'final', 'itinerary': {'summary': 'ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.'}}
