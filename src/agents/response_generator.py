"""
Response Generator Agent: ìµœì¢… ì‘ë‹µ ìƒì„± ì—ì´ì „íŠ¸ (Powered by Gemini)
"""
import logging
from typing import Dict, Any, List
from datetime import datetime

import os
try:
    # Optional LLM binding (only loaded when ENABLE_LLM=1)
    from langchain_google_genai import ChatGoogleGenerativeAI
except Exception:
    ChatGoogleGenerativeAI = None


logger = logging.getLogger(__name__)

class ResponseGeneratorAgent:
    def __init__(self):
        # LLM initialization is optional and gated by environment to keep tests
        # and CI lightweight. Set ENABLE_LLM=1 to attempt to use Google Gemini.
        self.llm = None
        self.enable_llm = os.getenv('ENABLE_LLM', '').lower() in ('1', 'true', 'yes') and ChatGoogleGenerativeAI is not None
        if self.enable_llm:
            try:
                self.llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0.7)
            except Exception:
                self.llm = None

        # Use a simple string template for prompt formatting to avoid hard
        # dependency on langchain prompt classes in tests.
        self.prompt_template = (
            "ë‹¹ì‹ ì€ ì „ë¬¸ ì—¬í–‰ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ì—¬í–‰ ê³„íšì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n"
            "**ì¤‘ìš”: ê° ì„¹ì…˜ í—¤ë”ì— ê´€ë ¨ ì´ëª¨ì§€ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:**\n"
            "- ìˆ™ì†Œ/í˜¸í…”: ğŸ¨\n"
            "- ë‚ ì”¨/ê¸°í›„: ğŸŒ¤ï¸ ë˜ëŠ” â˜€ï¸, ğŸŒ§ï¸, â„ï¸ (ë‚ ì”¨ì— ë”°ë¼)\n"
            "- ì¼ì •/ìŠ¤ì¼€ì¤„: ğŸ“…\n"
            "- ì•„ì¹¨ í™œë™: ğŸŒ…\n"
            "- ì ì‹¬ ì‹œê°„: ğŸ½ï¸\n"
            "- ì €ë… í™œë™: ğŸŒ†\n"
            "- êµí†µ/ì´ë™: ğŸš‡, ğŸšŒ, ğŸš¶\n"
            "- ìŒì‹/ë ˆìŠ¤í† ë‘: ğŸ´, ğŸ¥, ğŸ·\n"
            "- ê´€ê´‘ì§€/ëª…ì†Œ: ğŸ—¼, ğŸ›ï¸, â›ª\n"
            "- ë¬¸í™”/ì˜ˆìˆ : ğŸ¨, ğŸ­\n"
            "- ì‡¼í•‘: ğŸ›ï¸\n"
            "- ìì—°/ê³µì›: ğŸŒ³, ğŸï¸\n"
            "- íŒ/ì¡°ì–¸: ğŸ’¡\n\n"
            "ëª©ì ì§€: {destination}\n"
            "ì—¬í–‰ ë‚ ì§œ: {dates}\n"
            "ì—¬í–‰ ì¸ì›: {traveler_count}ëª…\n"
            "ì„ í˜¸ì‚¬í•­: {preferences}\n"
            "í˜¸í…” ê²€ìƒ‰ ê²°ê³¼: {hotel_results}\n"
            "ë‚ ì”¨ ì˜ˆë³´:\n{weather_forecast}\n"
            "êµ¬ê¸€ ê²€ìƒ‰ ì •ë³´: {google_info}\n\n"
            "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ëª¨ì§€ê°€ í¬í•¨ëœ ë§ì¶¤í˜• ì—¬í–‰ ê³„íšì„ ì‘ì„±í•´ì£¼ì„¸ìš”."
        )
        logger.info("ResponseGeneratorAgent(Gemini) ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def generate(self, state: Dict[str, Any]) -> Dict[str, Any]:
        try:
            destination = state.get('destination', 'ëª©ì ì§€ ë¯¸ì •')
            dates = state.get('travel_dates', ['ë‚ ì§œ ë¯¸ì •'])
            
            # ë‚ ì”¨ ì •ë³´ í¬ë§·íŒ…
            weather_info = self._format_weather_forecast(
                state.get('weather_forecast', []),
                state.get('context_memory', {}).get('weather_limitation_message')
            )
            
            prompt_text = self.prompt_template.format(
                destination=destination,
                dates=' ~ '.join(dates) if isinstance(dates, list) else dates,
                traveler_count=state.get('traveler_count', 1),
                preferences=str(state.get('preferences', {})),
                hotel_results=self._format_hotel_results(state.get('hotel_options', [])),
                weather_forecast=weather_info,
                google_info=str(state.get('google_search_results', []))
            )

            if self.llm:
                try:
                    response = await self.llm.ainvoke(prompt_text)
                    content = getattr(response, 'content', str(response))
                except Exception:
                    content = "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                # Lightweight fallback summary (non-LLM)
                content = f"{destination}ì— ëŒ€í•œ ê°„ë‹¨í•œ ì¼ì •ì…ë‹ˆë‹¤. í˜¸í…” {len(state.get('hotel_options', []))}ê³³ ì¶”ì²œ." 

            return {
                'destination': destination,
                'dates': dates,
                'summary': content,
                'hotels': [{'name': h.name, 'rating': h.rating, 'price': h.price_range, 'highlights': h.review_highlights} for h in state.get('hotel_options', [])[:3]],
                'generated_at': datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {'summary': f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", 'hotels': []}

    def _format_hotel_results(self, hotels: List) -> str:
        if not hotels: return "ê²€ìƒ‰ëœ í˜¸í…” ì—†ìŒ"
        return "\n".join([f"- {h.name} (í‰ì : {h.rating}, ê°€ê²©: {h.price_range})" for h in hotels[:3]])
    
    def _format_weather_forecast(self, forecasts: List, limitation_message: str = None) -> str:
        """
        ë‚ ì”¨ ì˜ˆë³´ë¥¼ Markdown í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        """
        if not forecasts:
            if limitation_message:
                return f"âš ï¸ {limitation_message}"
            return "ë‚ ì”¨ ì •ë³´ ì—†ìŒ (ë‚ ì§œê°€ 2ì£¼ ì´í›„ì´ê±°ë‚˜ ì¡°íšŒ ì‹¤íŒ¨)"
        
        table = "| ë‚ ì§œ | ë‚ ì”¨ | ìµœì €ê¸°ì˜¨ | ìµœê³ ê¸°ì˜¨ | ê°•ìˆ˜ëŸ‰ |\n"
        table += "|------|------|----------|----------|--------|\n"
        
        for forecast in forecasts:
            table += f"| {forecast.date} | {forecast.description} | {forecast.temperature_min}Â°C | {forecast.temperature_max}Â°C | {forecast.precipitation}mm |\n"
        
        return table

    async def stream_response(self, state: Dict[str, Any]):
        """Async generator that yields parts of the response incrementally.

        This is a lightweight PoC for streaming responses without invoking the
        LLM for every partial update. It yields three steps:
        1) hotels list
        2) weather table or message
        3) final summary (placeholder if LLM not available)
        """
        # 1) hotels
        hotels = [{'name': h.name, 'rating': h.rating, 'price': h.price_range, 'highlights': h.review_highlights} for h in state.get('hotel_options', [])[:3]]
        yield {'step': 'hotels', 'hotels': hotels}

        # 2) weather
        weather_info = self._format_weather_forecast(
            state.get('weather_forecast', []),
            state.get('context_memory', {}).get('weather_limitation_message')
        )
        yield {'step': 'weather', 'weather': weather_info}

        # 3) final summary â€” try LLM generate, but fall back to a simple placeholder
        try:
            final = await self.generate(state)
            yield {'step': 'final', 'itinerary': final}
        except Exception:
            # Non-fatal fallback
            yield {'step': 'final', 'itinerary': {'summary': 'ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.'}}
