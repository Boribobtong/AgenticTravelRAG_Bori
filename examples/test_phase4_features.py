"""
Phase 4 ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

êµ¬í˜„ëœ Phase 4 ê¸°ëŠ¥ë“¤ì„ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
"""

import asyncio
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.tools.ab_testing import ABTestingManager
from src.tools.satisfaction_tracker import SatisfactionTracker
from src.tools.metrics_collector import get_metrics_collector
from src.tools.retraining_pipeline import RetrainingPipeline


def test_ab_testing():
    """A/B í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("1. A/B í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    ab_manager = ABTestingManager()
    
    # ì‹¤í—˜ ìƒì„±
    try:
        experiment = ab_manager.create_experiment(
            name="test_experiment",
            description="í…ŒìŠ¤íŠ¸ ì‹¤í—˜",
            variants=[
                {"name": "variant_a", "config": {"alpha": 0.3}},
                {"name": "variant_b", "config": {"alpha": 0.7}}
            ]
        )
        print(f"âœ“ ì‹¤í—˜ ìƒì„± ì„±ê³µ: {experiment.name}")
        
        # ì‹¤í—˜ ì‹œì‘
        ab_manager.start_experiment("test_experiment")
        print(f"âœ“ ì‹¤í—˜ ì‹œì‘ë¨")
        
        # ì‚¬ìš©ìì—ê²Œ ë³€í˜• í• ë‹¹
        for i in range(5):
            user_id = f"user_{i}"
            variant = ab_manager.assign_variant("test_experiment", user_id)
            print(f"  - {user_id}: {variant['variant_name']} (alpha={variant['config']['alpha']})")
        
        # ê²°ê³¼ ê¸°ë¡
        for i in range(5):
            user_id = f"user_{i}"
            ab_manager.record_result(
                "test_experiment",
                user_id,
                {"satisfaction": 80 + i * 2}
            )
        print(f"âœ“ ê²°ê³¼ ê¸°ë¡ ì™„ë£Œ")
        
        # ê²°ê³¼ ë¶„ì„
        analysis = ab_manager.analyze_results("test_experiment")
        print(f"âœ“ ë¶„ì„ ê²°ê³¼:")
        for variant_name, stats in analysis['variants'].items():
            print(f"  - {variant_name}: {stats['sample_size']}ê°œ ìƒ˜í”Œ")
        
    except Exception as e:
        print(f"âœ— ì˜¤ë¥˜ ë°œìƒ: {e}")


def test_satisfaction_tracking():
    """ë§Œì¡±ë„ ì¶”ì  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("2. ë§Œì¡±ë„ ì¶”ì  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    tracker = SatisfactionTracker()
    
    try:
        # ëª…ì‹œì  í”¼ë“œë°± ê¸°ë¡
        tracker.record_explicit_feedback(
            session_id="test_session_1",
            feedback_type="thumbs_up"
        )
        print("âœ“ ëª…ì‹œì  í”¼ë“œë°± ê¸°ë¡ (thumbs_up)")
        
        # ì•”ë¬µì  ì‹ í˜¸ ê¸°ë¡
        tracker.record_implicit_signals(
            session_id="test_session_1",
            signals={
                'conversation_turns': 4,
                'search_refinements': 1,
                'hotels_viewed': 3,
                'weather_available': True,
                'time_to_completion': 5.5
            }
        )
        print("âœ“ ì•”ë¬µì  ì‹ í˜¸ ê¸°ë¡")
        
        # ë§Œì¡±ë„ ì ìˆ˜ ê³„ì‚°
        score = tracker.calculate_satisfaction_score("test_session_1")
        print(f"âœ“ ë§Œì¡±ë„ ì ìˆ˜: {score:.1f}/100")
        
        # í‰ê·  ë§Œì¡±ë„
        avg_score = tracker.get_avg_satisfaction(days=7)
        print(f"âœ“ ìµœê·¼ 7ì¼ í‰ê·  ë§Œì¡±ë„: {avg_score:.1f}/100")
        
    except Exception as e:
        print(f"âœ— ì˜¤ë¥˜ ë°œìƒ: {e}")


def test_metrics_collector():
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("3. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    metrics = get_metrics_collector()
    
    try:
        # ë…¸ë“œ ì‹¤í–‰ ì‹œê°„ ì¶”ì 
        with metrics.track_node_execution('test_node'):
            import time
            time.sleep(0.1)  # 0.1ì´ˆ ëŒ€ê¸°
        print("âœ“ ë…¸ë“œ ì‹¤í–‰ ì‹œê°„ ì¶”ì  ì™„ë£Œ")
        
        # ê²€ìƒ‰ í’ˆì§ˆ ê¸°ë¡
        metrics.record_search_quality(
            search_type='hotel',
            result_count=5,
            avg_score=0.85
        )
        print("âœ“ ê²€ìƒ‰ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê¸°ë¡")
        
        # ë§Œì¡±ë„ ì ìˆ˜ ê¸°ë¡
        metrics.record_satisfaction(87.5)
        print("âœ“ ë§Œì¡±ë„ ì ìˆ˜ ê¸°ë¡")
        
        # A/B ë³€í˜• í• ë‹¹ ê¸°ë¡
        metrics.record_ab_assignment('test_experiment', 'variant_a')
        print("âœ“ A/B ë³€í˜• í• ë‹¹ ê¸°ë¡")
        
        # ë©”íŠ¸ë¦­ ì¶œë ¥
        metrics_output = metrics.get_metrics().decode('utf-8')
        print(f"âœ“ Prometheus ë©”íŠ¸ë¦­ ìƒì„± ({len(metrics_output)} bytes)")
        
        # ì¼ë¶€ ë©”íŠ¸ë¦­ ì¶œë ¥
        print("\në©”íŠ¸ë¦­ ìƒ˜í”Œ:")
        for line in metrics_output.split('\n')[:10]:
            if line and not line.startswith('#'):
                print(f"  {line}")
        
    except Exception as e:
        print(f"âœ— ì˜¤ë¥˜ ë°œìƒ: {e}")


async def test_retraining_pipeline():
    """ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("4. ìë™ ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    pipeline = RetrainingPipeline()
    
    try:
        # ì¬í•™ìŠµ íŠ¸ë¦¬ê±° í™•ì¸
        triggers = pipeline.check_retraining_triggers()
        print("âœ“ ì¬í•™ìŠµ íŠ¸ë¦¬ê±° í™•ì¸:")
        for trigger_name, is_active in triggers.items():
            status = "ğŸ”´ í™œì„±" if is_active else "âšª ë¹„í™œì„±"
            print(f"  - {trigger_name}: {status}")
        
        # ì¬í•™ìŠµ í•„ìš” ì—¬ë¶€
        should_retrain = pipeline.should_retrain()
        print(f"\nâœ“ ì¬í•™ìŠµ í•„ìš”: {'ì˜ˆ' if should_retrain else 'ì•„ë‹ˆì˜¤'}")
        
        # ì¬í•™ìŠµ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸)
        if should_retrain:
            result = await pipeline.execute_retraining()
            print(f"âœ“ ì¬í•™ìŠµ ì‹¤í–‰ ê²°ê³¼: {result['status']}")
        
    except Exception as e:
        print(f"âœ— ì˜¤ë¥˜ ë°œìƒ: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("Phase 4 Production Ready ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # 1. A/B í…ŒìŠ¤íŒ…
    test_ab_testing()
    
    # 2. ë§Œì¡±ë„ ì¶”ì 
    test_satisfaction_tracking()
    
    # 3. ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    test_metrics_collector()
    
    # 4. ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸
    asyncio.run(test_retraining_pipeline())
    
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ì‹¤í–‰:")
    print("   docker-compose -f docker-compose.monitoring.yml up -d")
    print("   streamlit run src/tools/monitoring_dashboard.py")
    print("\n2. ì „ì²´ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰:")
    print("   pytest tests/unit/test_ab_testing.py -v")
    print("   pytest tests/unit/test_satisfaction_tracker.py -v")
    print("   pytest tests/unit/test_metrics_collector.py -v")
    print("   pytest tests/unit/test_retraining_pipeline.py -v")
    print()


if __name__ == "__main__":
    main()
