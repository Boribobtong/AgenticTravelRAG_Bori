import os
import json
import requests
from pathlib import Path
from loguru import logger
from dotenv import load_dotenv
import sys
from concurrent.futures import ProcessPoolExecutor
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (pip install tqdm)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.rag.elasticsearch_rag import get_rag_instance, ReviewDocument

load_dotenv("config/.env")

DATA_DIR = Path("data/raw")
INPUT_FILE = DATA_DIR / "tripadvisor_reviews.jsonl"

# ---------------------------------------------------------
# 1. í—¬í¼ í•¨ìˆ˜ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ (ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ ìµœìƒìœ„ ë ˆë²¨ì— ìœ„ì¹˜í•´ì•¼ í•¨)
# ---------------------------------------------------------
def parse_hotel_info_from_url(url):
    try:
        if 'Reviews-' not in url:
            return "Unknown Hotel", "Unknown Location"
        slug = url.split('Reviews-')[1].replace('.html', '')
        if '-' in slug:
            parts = slug.split('-', 1)
            hotel_name = parts[0].replace('_', ' ')
            location = parts[1].replace('_', ' ')
        else:
            hotel_name = slug.replace('_', ' ')
            location = "Unknown"
        return hotel_name, location
    except Exception:
        return "Unknown Hotel", "Unknown Location"

def extract_tags(property_dict, text):
    tags = []
    if property_dict:
        key_map = {
            'cleanliness': 'clean',
            'service': 'good_service',
            'location': 'good_location',
            'value': 'good_value',
            'sleep quality': 'quiet',
            'rooms': 'nice_rooms'
        }
        for key, score in property_dict.items():
            if float(score) >= 4.0 and key in key_map:
                tags.append(key_map[key])

    text_lower = text.lower()
    keywords = {
        'romantic': 'romantic', 'honeymoon': 'romantic',
        'family': 'family', 'kids': 'family', 'business': 'business',
        'solo': 'solo_travel', 'pool': 'pool', 'beach': 'beach_front',
        'breakfast': 'breakfast'
    }
    for word, tag in keywords.items():
        if word in text_lower:
            tags.append(tag)
    return list(set(tags))

# ---------------------------------------------------------
# 2. ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë‹¨ìœ„ ì‘ì—… í•¨ìˆ˜ ì •ì˜
# ---------------------------------------------------------
def process_single_line(line_data):
    """
    í•œ ì¤„(JSON ë¬¸ìì—´)ì„ ë°›ì•„ ReviewDocument ê°ì²´(ë˜ëŠ” None)ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    ì´ í•¨ìˆ˜ëŠ” ê° ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """
    try:
        # tuple (idx, line_string) í˜•íƒœë¡œ ë°›ìŒ
        idx, line = line_data
        item = json.loads(line)
        
        hotel_name, location = parse_hotel_info_from_url(item.get('hotel_url', ''))
        tags = extract_tags(item.get('property_dict', {}), item.get('text', ''))

        doc = ReviewDocument(
            doc_id=f"review_{idx}",
            hotel_name=hotel_name,
            location=location,
            review_text=item.get('text', ''),
            rating=float(item.get('rating', 0)),
            review_title=item.get('title', ''),
            tags=tags,
            reviewer_location=item.get('author', '')
        )
        return doc
    except Exception:
        return None

# ---------------------------------------------------------
# 3. ë©”ì¸ ì¸ë±ì‹± ë¡œì§ ê°œì„ 
# ---------------------------------------------------------
def index_data():
    print("\n" + "="*60)
    print("ğŸš€ ElasticSearch ëŒ€ìš©ëŸ‰ ë³‘ë ¬ ì¸ë±ì‹± (Optimized)")
    print("="*60 + "\n")
    
    rag = get_rag_instance()
    
    # ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    rag.create_index(force_recreate=True)
    
    if not INPUT_FILE.exists():
        print(f"âŒ ë°ì´í„° íŒŒì¼ ì—†ìŒ: {INPUT_FILE}")
        return

    # ì„¤ì •ê°’ ì¡°ì •
    BATCH_SIZE = 5000  # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (500 -> 5000)
    MAX_WORKERS = max(1, os.cpu_count() - 1)  # CPU ì½”ì–´ ìˆ˜ í™œìš© (í•˜ë‚˜ ë‚¨ê²¨ë‘ )

    print(f"âš™ï¸  ì„¤ì •: Batch Size={BATCH_SIZE}, Workers={MAX_WORKERS}")
    
    # ì „ì²´ ë¼ì¸ ìˆ˜ ê³„ì‚° (tqdm ì§„í–‰ë¥  í‘œì‹œìš©, 100ë§Œê°œë©´ ì•½ê°„ ì‹œê°„ ê±¸ë¦¼)
    print("ğŸ“Š ì „ì²´ ë¼ì¸ ìˆ˜ ê³„ì‚° ì¤‘...")
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        total_lines = sum(1 for _ in f)
    
    documents_batch = []
    
    # ProcessPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        # ì¸ë±ìŠ¤ì™€ ë¼ì¸ì„ íŠœí”Œë¡œ ë¬¶ì–´ì„œ generator ìƒì„±
        lines_gen = ((i, line) for i, line in enumerate(f))
        
        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # tqdmìœ¼ë¡œ ì§„í–‰ìƒí™© í‘œì‹œ
            results = tqdm(
                executor.map(process_single_line, lines_gen), 
                total=total_lines,
                unit="docs",
                desc="Processing & Indexing"
            )
            
            for doc in results:
                if doc:
                    documents_batch.append(doc)
                
                # ë°°ì¹˜ê°€ ì°¨ë©´ ì¸ë±ì‹± ì‹¤í–‰ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ìˆ˜í–‰)
                if len(documents_batch) >= BATCH_SIZE:
                    # [ìˆ˜ì •] use_dummy_embedding=True ì˜µì…˜ ì¶”ê°€ (ì†ë„ ìµœì í™” ëª¨ë“œ)
                    # ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© ì‹œ use_dummy_embedding=Falseë¡œ ë³€ê²½
                    rag.index_documents(documents_batch, batch_size=BATCH_SIZE, use_dummy_embedding=False)
                    documents_batch = [] # ë¹„ìš°ê¸°

    # ë‚¨ì€ ë¬¸ì„œ ì²˜ë¦¬
    if documents_batch:
        # [ìˆ˜ì •] use_dummy_embedding=True ì˜µì…˜ ì¶”ê°€
        # ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© ì‹œ use_dummy_embedding=Falseë¡œ ë³€ê²½
        rag.index_documents(documents_batch, batch_size=BATCH_SIZE, use_dummy_embedding=False)
    
    print(f"\nâœ… ì¸ë±ì‹± ì™„ë£Œ!")

if __name__ == "__main__":
    # Windows/Mac í™˜ê²½ì˜ Multiprocessing ë³´í˜¸ë¥¼ ìœ„í•´ í•„ìˆ˜
    index_data()