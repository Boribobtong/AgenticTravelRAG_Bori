"""
ë°ì´í„° ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

HuggingFace datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ 
TripAdvisor ë¦¬ë·° ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python -m data.scripts.download_data
    ë˜ëŠ”
    python data/scripts/download_data.py
"""

import os
import json
import yaml
from pathlib import Path
from datasets import load_dataset
from loguru import logger
from dotenv import load_dotenv
from pandas import Timestamp 

# í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • ë¡œë“œ
load_dotenv()
try:
    with open("config/config.yaml", 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
except FileNotFoundError:
    logger.error("config/config.yaml íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)

DATA_CONFIG = config.get('data', {})
DATASET_NAME = DATA_CONFIG.get('huggingface_dataset')
RAW_DIR = Path(DATA_CONFIG.get('raw_dir'))
MAX_DOCS = DATA_CONFIG.get('max_docs_for_dev')

def _jsonable(value):
    if isinstance(value, Timestamp):
        return value.isoformat()
    return value

def download_data():
    """ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  JSONL í˜•ì‹ìœ¼ë¡œ raw ë””ë ‰í† ë¦¬ì— ì €ì¥"""
    
    print("\n" + "="*60)
    print("ğŸ”½ TripAdvisor ë¦¬ë·° ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
    print("="*60 + "\n")
    
    if not DATASET_NAME:
        logger.error("HuggingFace ë°ì´í„°ì…‹ ì´ë¦„ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("âŒ ì„¤ì • ì˜¤ë¥˜: config/config.yamlì„ í™•ì¸í•˜ì„¸ìš”.\n")
        return

    os.makedirs(RAW_DIR, exist_ok=True)
    output_file = RAW_DIR / "tripadvisor_reviews.jsonl"

    if output_file.exists():
        logger.info(f"ë°ì´í„° íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {output_file}. ë‹¤ìš´ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        print(f"â„¹ï¸  ë°ì´í„° íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {output_file}")
        print("   ì¬ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´ íŒŒì¼ì„ ì‚­ì œí•˜ì„¸ìš”.\n")
        return

    logger.info(f"ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {DATASET_NAME}")
    print(f"ğŸ“¦ ë°ì´í„°ì…‹: {DATASET_NAME}")
    
    try:
        # ë°ì´í„°ì…‹ ë¡œë“œ (train split ì‚¬ìš©)
        print("â³ HuggingFaceì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
        dataset = load_dataset(DATASET_NAME, split="train")
        
        # ê°œë°œ í™˜ê²½ì„ ìœ„í•´ ë¬¸ì„œ ìˆ˜ ì œí•œ
        if MAX_DOCS and len(dataset) > MAX_DOCS:
            dataset = dataset.select(range(MAX_DOCS))
            print(f"ğŸ“Š ê°œë°œ ëª¨ë“œ: {MAX_DOCS}ê°œ ë¬¸ì„œë¡œ ì œí•œ")

        # JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
        print(f"ğŸ’¾ ì €ì¥ ì¤‘: {output_file}")
        with open(output_file, "w", encoding="utf-8") as f:
            for item in dataset:
                clean_item = {k: _jsonable(v) for k, v in item.items()}
                f.write(json.dumps(clean_item, ensure_ascii=False) + "\n")

        logger.success(f"ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì €ì¥ ì™„ë£Œ: {output_file} ({len(dataset)}ê°œ ë¬¸ì„œ)")
        print(f"\nâœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        print(f"   íŒŒì¼: {output_file}")
        print(f"   ë¬¸ì„œ ìˆ˜: {len(dataset):,}ê°œ\n")
        print("="*60 + "\n")

    except Exception as e:
        logger.error(f"ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        print(f"\nâŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}\n")
        print("="*60 + "\n")

if __name__ == "__main__":
    download_data()